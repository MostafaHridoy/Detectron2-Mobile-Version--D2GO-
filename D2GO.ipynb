{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "g7qGaMvcGgRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre torch torchvision torchtext -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html -U\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "id": "g_JK696uHFee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/mobile-vision.git'\n",
        "!git clone https://github.com/facebookresearch/d2go\n",
        "!pip install \"/content/d2go\"\n",
        "!pip install 'git+https://github.com/facebookresearch/mobile-vision.git'\n",
        "!pip install pyyaml==5.4\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "2wYJHq4QHO_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "print(torch.version, torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsA1Je9HHTb1",
        "outputId": "e17b130f-1a8b-4d7d-ce8e-ced31c4a8cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'torch.version' from '/usr/local/lib/python3.9/dist-packages/torch/version.py'> True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -av '/content/d2go/configs' '/usr/local/lib/python3.9/dist-packages/d2go/'"
      ],
      "metadata": {
        "id": "43Y1TH3tHVVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from d2go.model_zoo import model_zoo\n",
        "model = model_zoo.get('mask_rcnn_fbnetv3a_C4.yaml', trained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOx32HfiHYBG",
        "outputId": "9db52e15-dd62-45d5-d9cd-99f96346ec32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:mobile_cv.arch.utils.helper:Arguments ['width_divisor', 'dw_skip_bnrelu', 'zero_last_bn_gamma'] skipped for op Conv2d\n",
            "model_final.pth: 74.3MB [00:06, 11.0MB/s]                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcRd1IpD15Qv",
        "outputId": "536db256-616b-416c-a945-733770307410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
        "im = cv2.imread(\"./input.jpg\")\n",
        "plt.imshow(im)"
      ],
      "metadata": {
        "id": "aQ6lYBBWHapf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from d2go.utils.demo_predictor import DemoPredictor\n",
        "predictor = DemoPredictor(model)\n",
        "outputs = predictor(im)\n",
        "# the output object categories and corresponding bounding boxes\n",
        "print(outputs[\"instances\"].pred_classes)\n",
        "print(outputs[\"instances\"].pred_boxes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Zka4iqHdqf",
        "outputId": "f2d33a7f-fefa-4567-e328-c2765f856536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([17,  0,  0, 17, 17, 17,  0, 20, 19,  0, 17, 20, 17, 22, 19, 19, 19],\n",
            "       device='cuda:0')\n",
            "Boxes(tensor([[123.8110, 241.3701, 479.6925, 480.0000],\n",
            "        [276.2808, 147.8152, 365.5569, 400.5892],\n",
            "        [ 51.7697, 279.1486,  78.7433, 341.4369],\n",
            "        [504.6472, 261.5977, 587.4645, 335.1068],\n",
            "        [331.7795, 234.2218, 410.3190, 307.9734],\n",
            "        [408.7300, 285.3493, 462.2014, 353.3682],\n",
            "        [  2.9709, 277.4141,  75.1721, 476.9530],\n",
            "        [  3.1935, 277.9557,  75.0489, 476.4275],\n",
            "        [504.8673, 262.9518, 592.4090, 339.0172],\n",
            "        [556.2568, 270.3552, 595.2379, 355.6459],\n",
            "        [  3.2859, 277.9533,  75.0428, 476.7831],\n",
            "        [504.8312, 262.8539, 587.5267, 330.3096],\n",
            "        [555.7526, 280.5878, 595.2996, 360.6277],\n",
            "        [555.3569, 279.8503, 595.3765, 360.5094],\n",
            "        [595.2652, 266.1639, 628.4905, 333.6963],\n",
            "        [  2.7089, 277.7745,  75.3923, 476.8129],\n",
            "        [555.5452, 276.6618, 598.2251, 355.9480]], device='cuda:0'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(\"coco_2017_train\"))\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "plt.imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "GwmI-ApiHfkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DatasetCatalog.clear()"
      ],
      "metadata": {
        "id": "3gqkazsTHig2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for 500 train images"
      ],
      "metadata": {
        "id": "OPylDOE2ZJlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"my_dataset_train\", {}, \"train .json file\", \"train image dataset link\")\n",
        "register_coco_instances(\"my_dataset_val\", {}, \"validation.json file\", \"valid_images link\")'''"
      ],
      "metadata": {
        "id": "QfW3dKfqHmnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize training data\n",
        "from detectron2.utils.visualizer import Visualizer"
      ],
      "metadata": {
        "id": "HSI036LcH49A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataset_val_metadata = MetadataCatalog.get(\"my_dataset_val\")\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset_val\")\n",
        "\n",
        "import random\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset_val\")\n",
        "for d in random.sample(dataset_dicts, 5):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_val_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    plt.figure()\n",
        "    plt.imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "lZ1ZbXISH9C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize training data\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "my_dataset_train_metadata = MetadataCatalog.get(\"my_dataset_train\")\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset_train\")\n",
        "\n",
        "import random\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset_train\")\n",
        "for d in random.sample(dataset_dicts, 5):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    plt.figure()\n",
        "    plt.imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "k0ojMLn_H9wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wep_train_metadata = MetadataCatalog.get(\"my_dataset_train\")\n",
        "wep_train_metadata"
      ],
      "metadata": {
        "id": "mOQBMWWDID2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wep_val_metadata = MetadataCatalog.get(\"my_dataset_val\")\n",
        "wep_val_metadata"
      ],
      "metadata": {
        "id": "uYDpBoViIKpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from d2go.runner import GeneralizedRCNNRunner"
      ],
      "metadata": {
        "id": "0DxZr2rEIMjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_for_launch():\n",
        "    runner = GeneralizedRCNNRunner()\n",
        "    cfg = runner.get_default_cfg()\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"faster_rcnn_fbnetv3a_C4.yaml\"))\n",
        "    cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "    cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
        "\n",
        "    cfg.DATALOADER.NUM_WORKERS = 2\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"faster_rcnn_fbnetv3a_C4.yaml\")  # Let training initialize from model zoo\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 1\n",
        "    cfg.SOLVER.BASE_LR = 0.0001\n",
        "\n",
        "\n",
        "    cfg.SOLVER.WARMUP_ITERS = 20\n",
        "    cfg.SOLVER.MAX_ITER = 500 #adjust up if val mAP is still rising, adjust down if overfit\n",
        "    cfg.SOLVER.STEPS = []\n",
        "    cfg.SOLVER.GAMMA = 0.05\n",
        "\n",
        "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2\n",
        "\n",
        "    cfg.TEST.EVAL_PERIOD = 10\n",
        "    return cfg, runner\n",
        "\n",
        "cfg, runner = prepare_for_launch()\n",
        "model = runner.build_model(cfg)\n",
        "runner.do_train(cfg, model, resume=False)"
      ],
      "metadata": {
        "id": "CtFMk_njISH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = runner.do_test(cfg, model)"
      ],
      "metadata": {
        "id": "d4E2PFSFIT4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics)"
      ],
      "metadata": {
        "id": "Whb0ihW-Iw73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from detectron2.data import build_detection_test_loader\n",
        "from d2go.export.exporter import convert_and_export_predictor\n",
        "from d2go.utils.testing.data_loader_helper import create_detection_data_loader_on_toy_dataset\n",
        "\n",
        "import logging\n",
        "\n",
        "# disable all the warnings\n",
        "previous_level = logging.root.manager.disable\n",
        "logging.disable(logging.INFO)\n",
        "\n",
        "cfg_name = 'faster_rcnn_fbnetv3a_C4.yaml'\n",
        "pytorch_model = model_zoo.get(cfg_name, trained=True, device='cpu')\n",
        "pytorch_model.eval()\n",
        "cfg = model_zoo.get_config(cfg_name)\n",
        "\n",
        "with create_detection_data_loader_on_toy_dataset(cfg, 224, 320, is_train=False) as data_loader:\n",
        "    predictor_path = convert_and_export_predictor(\n",
        "            cfg,\n",
        "            copy.deepcopy(pytorch_model),\n",
        "            \"torchscript_int8\",\n",
        "            './',\n",
        "            data_loader,\n",
        "        )\n",
        "\n",
        "# recover the logging level\n",
        "logging.disable(previous_level)"
      ],
      "metadata": {
        "id": "Ra9q4QQqIxeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from d2go.utils.demo_predictor import DemoPredictor\n",
        "\n",
        "predictor = DemoPredictor(model)\n",
        "\n",
        "dataset_dicts = DatasetCatalog.get('my_dataset_val')\n",
        "for i in random.sample(dataset_dicts, 5):\n",
        "    im = cv2.imread(i[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1], metadata=MetadataCatalog.get(\"my_dataset_val\"), scale=0.8)\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    plt.figure(figsize = (14, 10))\n",
        "    plt.imshow(cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OyiTepJ5I0Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "import torch\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "import os\n",
        "\n",
        "predictor_path = \"/content/torchscript_int8\"\n",
        "\n",
        "class Wrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        coco_idx_list = [1, 2, 3, 4, 5]\n",
        "        self.coco_idx = torch.tensor(coco_idx_list)\n",
        "\n",
        "    def forward(self, inputs: List[torch.Tensor]):\n",
        "        x = inputs[0].unsqueeze(0) * 255\n",
        "        scale = 320.0 / min(x.shape[-2], x.shape[-1])\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=scale, mode=\"bilinear\", align_corners=True, recompute_scale_factor=True)\n",
        "        out = self.model(x[0])\n",
        "        res : Dict[str, torch.Tensor] = {}\n",
        "        res[\"boxes\"] = out[0] / scale\n",
        "        res[\"labels\"] = torch.index_select(self.coco_idx, 0, out[1])\n",
        "        res[\"masks\"] = out[2]\n",
        "        res[\"scores\"] = out[3]\n",
        "        return inputs, [res]\n",
        "\n",
        "orig_model = torch.jit.load(os.path.join(predictor_path, \"model.jit\"))\n",
        "wrapped_model = Wrapper(orig_model)\n",
        "scripted_model = torch.jit.script(wrapped_model)\n",
        "# scripted_model.save(\"d2go_mask.pt\")\n",
        "optimized_scripted_module = optimize_for_mobile(scripted_model)\n",
        "\n",
        "#the name of the converted model to be used for android\n",
        "optimized_scripted_module._save_for_lite_interpreter(\"d2go_josh_faster.ptl\")"
      ],
      "metadata": {
        "id": "MV6KVV0rI5OO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}